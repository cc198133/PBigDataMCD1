{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6j7Hs4oZni0"
   },
   "source": [
    "#Código que introduce operaciones que te Sirve para aprender manipulación cluster\n",
    "\n",
    "# Fundamentos de Apache Spark: RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx34lYJ_Zni1"
   },
   "source": [
    "En este notebook trabajaremos con los RDDs que forma parte del Spark Core.La implementación de Spark Core es un **RDD (Resilient Distributed Dataset)** que es una colección de datos distribuidos en diferentes nodos del clúster que se procesan en paralelo.\n",
    "\n",
    "Utilizaremos la API de PySpark, pero los conceptos aplican por igual a todas las APIs (Scala, R, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsI8-MPeZni1"
   },
   "source": [
    "### Inicialización de Spark en Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjRi-orVZni2"
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "error",
     "timestamp": 1701309486707,
     "user": {
      "displayName": "EDUARDO CARLOS FLETES ARECHIGA",
      "userId": "11137692996056623365"
     },
     "user_tz": 360
    },
    "id": "Tw0lAU-2Zni2",
    "outputId": "e2b19c66-860f-4139-b38b-b85dc48a8923"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7c56fb6583b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnP67ROgZni2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bswwH5_MZni2"
   },
   "source": [
    "### Crear el SparkSession y el SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK6M3KG2Zni2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_training')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdfRR0r2Zni3"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCNPUuW1Zni3"
   },
   "source": [
    "### Crear un RDD de una colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41xpVZf1Zni3",
    "outputId": "7631661f-92d4-4c10-c9d6-74a38ec16ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = [1,2,3,4,5]\n",
    "\n",
    "num_rdd = sc.parallelize(num)\n",
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xS9bf-uZni3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQPnX9q2Zni4"
   },
   "source": [
    "# Transformaciones\n",
    "* Como sabemos, las Transformaciones son de naturaleza perezosa y no se ejecutarán hasta que se ejecute una Acción sobre ellas.\n",
    "* Intentemos comprender las distintas transformaciones disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WByTkcDZni4"
   },
   "source": [
    "### map\n",
    "* Esto mapeará su entrada a alguna salida basada en la función especificada en la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYAs-l0ZZni4",
    "outputId": "255f1c2d-129e-4b97-970a-11e58039532f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_rdd = num_rdd.map(lambda x : x * 2)\n",
    "double_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzZMW4_9Zni4"
   },
   "source": [
    "### filtro\n",
    "* Para filtrar los datos en función de una determinada condición. Intentemos encontrar los números pares de num_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jIUe6izZni4",
    "outputId": "49e7d618-2e60-42a2-99ae-247654146789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_rdd = num_rdd.filter(lambda x : x % 2 == 0)\n",
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGZ73JraZni4"
   },
   "source": [
    "### flatMap\n",
    "* Esta función es muy similar a map, pero puede devolver múltiples elementos para cada entrada en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9lT1hAQZni4",
    "outputId": "c3d8749c-089f-4f31-c085-e21e268d577b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_rdd = num_rdd.flatMap(lambda x : range(1,x))\n",
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBRD6LdtZni4"
   },
   "source": [
    "### distinct\n",
    "* Esto devolverá elementos distintos de un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d8tFzCmZni5",
    "outputId": "add2e2d8-2b81-4b27-bebc-3238d549d144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([10, 11, 10, 11, 12, 11])\n",
    "dist_rdd = rdd1.distinct()\n",
    "dist_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT8KF5F6Zni5"
   },
   "source": [
    "### reduceByKey\n",
    "* Esta función reduce los pares de valores clave en función de las claves y una función determinada dentro de reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBUjypzsZni5",
    "outputId": "255ee963-7598-468e-cdc6-c1242838c070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 8)\n",
      "('b', 8)\n",
      "('c', 6)\n"
     ]
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"b\", 7), (\"c\", 2), (\"a\", 3), (\"b\", 1), (\"c\", 4)]\n",
    "pair_rdd = sc.parallelize(pairs)\n",
    "\n",
    "output = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "result = output.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjaSxbCYZni5"
   },
   "source": [
    "### groupByKey\n",
    "* Esta función es otra función ByKey que puede operar en un par (clave, valor) RDD pero esto solo agrupará los valores basados en las claves. En otras palabras, esto solo realizará el primer paso de reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIxJZy-RZni5",
    "outputId": "6c40b61a-0639-404c-efbe-ca7d10d78bc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x1cc3ebdd2e0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x1cc2e1d5100>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x1cc2f007e50>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_out = pair_rdd.groupByKey()\n",
    "grp_out.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDwNRCjIZni5"
   },
   "source": [
    "### sortByKey\n",
    "* Esta función realizará la clasificación en un par (clave, valor) RDD basado en las claves. De forma predeterminada, la clasificación se realizará en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClvZCnlOZni5",
    "outputId": "1c5fac9d-6b88-46e5-87a7-ab6a2985dad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 5)\n",
      "('b', 3)\n",
      "('c', 2)\n",
      "('d', 7)\n"
     ]
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\", 3)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "sortkey_rdd = raw_rdd.sortByKey()\n",
    "result = sortkey_rdd.collect()\n",
    "print(*result,sep='\\n')\n",
    "\n",
    "# Para clasificar en orden descendente, pase  “ascending=False”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7e8L-iQZni5"
   },
   "source": [
    "### Ordenar por\n",
    "* sortBy es una función más generalizada para ordenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RxxGBt2Zni6",
    "outputId": "a642d5ec-897e-41a9-c3d7-61c955db7faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', 3, 9)\n",
      "('a', 5, 10)\n",
      "('c', 2, 11)\n",
      "('d', 7, 12)\n"
     ]
    }
   ],
   "source": [
    "# Create RDD.\n",
    "pairs = [ (\"a\", 5, 10), (\"d\", 7, 12), (\"c\", 2, 11), (\"b\", 3, 9)]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# Let’s try to do the sorting based on the 3rd element of the tuple.\n",
    "sort_out = raw_rdd.sortBy(lambda x : x[2])\n",
    "result = sort_out.collect()\n",
    "print(*result, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fRzTptDZni6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaQb6qwvZni6"
   },
   "source": [
    "# Acciones\n",
    "\n",
    "* Las acciones son operaciones en RDD que se ejecutan inmediatamente. Mientras que las transformaciones devuelven otro RDD, las acciones devuelven estructuras de datos nativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfPIiwYUZni6"
   },
   "source": [
    "### count\n",
    "* Esto contará el número de elementos en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ6jKdMSZni6",
    "outputId": "4959f398-9ac6-4932-b7af-ee6bde16008a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = sc.parallelize([1,2,3,4,2])\n",
    "num.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqDUUihTZni6"
   },
   "source": [
    "### first\n",
    "* Esto devolverá el primer elemento del RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehnTMxtnZni6",
    "outputId": "0d7dce8e-29a5-4711-d38e-5229d7ff3286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN2aJ2oBZni6"
   },
   "source": [
    "### Collect\n",
    "* Esto devolverá todos los elementos para el RDD dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMnon3TwZni6",
    "outputId": "0e044835-853a-404f-b3d0-74fc9e75566a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1CgsOCZni7"
   },
   "source": [
    "**No debemos utilizar la operación de collect mientras trabajamos con grandes conjuntos de datos**. Porque devolverá todos los datos que se distribuyen entre los diferentes trabajadores dl clúster a un controlador. Todos los datos viajarán a través de la red del trabajador al conductor y también el conductor necesitaría almacenar todos los datos. Esto obstaculizará el rendimiento de su aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2Q_4blIZni7"
   },
   "source": [
    "### Take\n",
    "* Esto devolverá el número de elementos especificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9LaU_BJZnjA",
    "outputId": "ba3d7083-5895-4444-d3fe-9c3034672b24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.take(3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
