{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDYTiqW5TYZq"
   },
   "source": [
    "# Con Spark SQL se puede acceder a diversas fuentes de datos, trabajar con datos estructurados y utilizar consultas de datos a través de SQL y otras API de DataFrame.\n",
    "\n",
    "# Fundamentos de Apache Spark: SQL/DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6Ys7cmsUiGR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnPJJXzjUiV5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKgHrskPTYZr"
   },
   "source": [
    "**Spark SQLtrabaja con DataFrames**. Un DataFrame es una **representación relacional de los datos**. Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos.\n",
    "\n",
    "Los DataFrames son similares a las tablas relacionales o DataFrames en Python / R auqnue con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario. Hay varias formas de crear DataFrames a partir de colecciones, tablas HIVE, tablas relacionales y RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt1FjfCRTYZs"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mo81n3gsTYZs"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KOjuEjRTYZs"
   },
   "source": [
    "### Crear la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8Eg2IU8TYZs"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ipWZiQQTYZs",
    "outputId": "1de2e905-7e57-4692-8e05-ebb32d4ad277"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://osvaldo:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2910b5069b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoOWuUF7TYZt"
   },
   "source": [
    "### Crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qm71HY9TYZt"
   },
   "outputs": [],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (8, \"HHH\", \"dept3\", 3700),\n",
    "    (9, \"III\", \"dept3\", 4500),\n",
    "    (10, \"JJJ\", \"dept5\", 3400)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI_o0BDOTYZt",
    "outputId": "b373187a-043e-4ad9-a363-49d9edd6da18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qozsMabSTYZt"
   },
   "outputs": [],
   "source": [
    "#Crear un df a partir de una tabla de Hive\n",
    "df = spark.table(“tbl_name”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNLrb4NcTYZu"
   },
   "source": [
    "# Operaciones básicas en DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ70WZBbTYZu"
   },
   "source": [
    "### count\n",
    "* Cuenta el número de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZrTigoxTYZu",
    "outputId": "6fb7ff47-02e2-48a6-d9b2-55b394ef2a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOQXu5nxTYZu"
   },
   "source": [
    "### columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI9aMtf7TYZu",
    "outputId": "370728bf-b511-4d37-a659-87265df30c4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'dept', 'salary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiPhishWTYZv"
   },
   "source": [
    "### dtypes\n",
    "** Accede al DataType de columnas dentro del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1MH-gHQTYZv",
    "outputId": "983667d2-65d7-40b5-972f-ac689d68108d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('name', 'string'),\n",
       " ('dept', 'string'),\n",
       " ('salary', 'bigint')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGCLglj5TYZv"
   },
   "source": [
    "### schema\n",
    "** Comprueba cómo Spark almacena el esquema del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygU8un90TYZv",
    "outputId": "246059a0-a3f0-4137-f002-9479079ff387"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k648AfQRTYZv"
   },
   "source": [
    "### printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jWxQSokTYZv",
    "outputId": "7fd8aa06-e32a-45ff-fbcb-6d2e477c919a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV5Zvf6KTYZv"
   },
   "source": [
    "### select\n",
    "* Seleccione columnas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgGff5waTYZv",
    "outputId": "9f08555b-4d98-44df-8f96-4fbba98e0543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1| AAA|\n",
      "|  2| BBB|\n",
      "|  3| CCC|\n",
      "|  4| DDD|\n",
      "|  5| EEE|\n",
      "|  6| FFF|\n",
      "|  7| GGG|\n",
      "|  8| HHH|\n",
      "|  9| III|\n",
      "| 10| JJJ|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDhAw5Z8TYZw"
   },
   "source": [
    "### filter\n",
    "\n",
    "* Filtrar las filas según alguna condición.\n",
    "* Intentemos encontrar las filas con id = 1.\n",
    "* Hay diferentes formas de especificar la condición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5IyEeOCTYZw",
    "outputId": "a8d06447-4429-46a5-893b-f01a9203a3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"id\"] == 1).show()\n",
    "df.filter(df.id == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwtD-w84TYZw",
    "outputId": "f114d5fd-db21-4cae-d92e-93433fabb658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n",
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"id\") == 1).show()\n",
    "df.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxnqpw7kTYZw"
   },
   "source": [
    "### drop\n",
    "* Elimina una columna en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVW6nHULTYZw",
    "outputId": "2667d4b1-d8b9-4440-9109-3200e96b03f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name| dept|salary|\n",
      "+----+-----+------+\n",
      "| AAA|dept1|  1000|\n",
      "| BBB|dept1|  1100|\n",
      "+----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.drop(\"id\")\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2zGTJ3kTYZw"
   },
   "source": [
    "### Aggregations\n",
    "* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK-f_P2oTYZw",
    "outputId": "6839dc15-a4ad-401c-bbd0-247df546fac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+------+\n",
      "| dept|count|  sum| max| min|   avg|\n",
      "+-----+-----+-----+----+----+------+\n",
      "|dept5|    1| 3400|3400|3400|3400.0|\n",
      "|dept3|    3|15300|7100|3700|5100.0|\n",
      "|dept1|    4| 6600|3000|1000|1650.0|\n",
      "|dept2|    2|15200|8000|7200|7600.0|\n",
      "+-----+-----+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"dept\")\n",
    "    .agg(\n",
    "        count(\"salary\").alias(\"count\"),\n",
    "        sum(\"salary\").alias(\"sum\"),\n",
    "        max(\"salary\").alias(\"max\"),\n",
    "        min(\"salary\").alias(\"min\"),\n",
    "        avg(\"salary\").alias(\"avg\")\n",
    "        ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xb2i7qfoTYZw"
   },
   "source": [
    "### Sorting\n",
    "\n",
    "* Ordena los datos según el \"salario\". De forma predeterminada, la clasificación se realizará en orden ascendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9N3JMX-vTYZx",
    "outputId": "91e3dfcd-8977-4976-f1c5-804f30d0a6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  3| CCC|dept1|  3000|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"salary\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXipIftPTYZx",
    "outputId": "81fceb64-bc9a-4d0b-d04c-b0bbc19c1c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  9| III|dept3|  4500|\n",
      "|  8| HHH|dept3|  3700|\n",
      "+---+----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data in descending order.\n",
    "df.sort(desc(\"salary\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz5VRkd5TYZx"
   },
   "source": [
    "### Columnas derivadas\n",
    "* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xfk-En4TYZx",
    "outputId": "037f8f32-cc74-4ffc-86db-e8558d19d177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+\n",
      "| id|name| dept|salary|bonus|\n",
      "+---+----+-----+------+-----+\n",
      "|  1| AAA|dept1|  1000|100.0|\n",
      "|  2| BBB|dept1|  1100|110.0|\n",
      "|  3| CCC|dept1|  3000|300.0|\n",
      "|  4| DDD|dept1|  1500|150.0|\n",
      "|  5| EEE|dept2|  8000|800.0|\n",
      "|  6| FFF|dept2|  7200|720.0|\n",
      "|  7| GGG|dept3|  7100|710.0|\n",
      "|  8| HHH|dept3|  3700|370.0|\n",
      "|  9| III|dept3|  4500|450.0|\n",
      "| 10| JJJ|dept5|  3400|340.0|\n",
      "+---+----+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KlAQl2eTYZx"
   },
   "source": [
    "### Joins\n",
    "\n",
    "* Podemos realizar varios tipos de combinaciones en múltiples DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKq2jMSqTYZx"
   },
   "source": [
    "![Sin%20t%C3%ADtulo.png](attachment:Sin%20t%C3%ADtulo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hC9PPrsbTYZx",
    "outputId": "a78feb7b-8338-45e6-bc15-0f1140a24316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner JOIN.\n",
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pMypSwZTYZy"
   },
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxD9bnxJTYZy",
    "outputId": "985000f8-8d9e-4da2-fd09-ddc5c7b82887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n",
      "| id|name| dept|salary|   id|          name|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "| 10| JJJ|dept5|  3400| null|          null|\n",
      "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|  9| III|dept3|  4500|dept3|Department - 3|\n",
      "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+---+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxWZw-SOTYZy"
   },
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMf7blwmTYZy",
    "outputId": "c88c16e0-20ea-4e81-f563-b9f1b717cd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn_WKdj7TYZy"
   },
   "source": [
    "### Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyqhcpJmTYZy",
    "outputId": "97a74961-e9e5-4736-d90d-9b0ee2c7cfce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n",
      "|  id|name| dept|salary|   id|          name|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "|  10| JJJ|dept5|  3400| null|          null|\n",
      "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
      "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
      "|   9| III|dept3|  4500|dept3|Department - 3|\n",
      "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
      "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
      "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
      "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
      "|null|null| null|  null|dept4|Department - 4|\n",
      "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
      "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
      "+----+----+-----+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut-_qwzjTYZz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqlMQsMATYZz"
   },
   "source": [
    "### Consultas SQL\n",
    "* Ejecución de consultas tipo SQL.\n",
    "* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdrk9VeATYZz",
    "outputId": "c720cc84-fc35-4b65-a554-0779c2924e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as Temporary Table\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Execute SQL-Like query.\n",
    "spark.sql(\"select * from temp_table where id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JXyF5ncTYZz",
    "outputId": "232f917c-c11e-4f8c-ec14-986ab0d0fd3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  6|\n",
      "|  9|\n",
      "|  5|\n",
      "|  1|\n",
      "| 10|\n",
      "|  3|\n",
      "|  8|\n",
      "|  2|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct id from temp_table\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euaWQGqZTYZz",
    "outputId": "0bdff5c6-88c8-495c-d999-8adb41bd8534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "|  8| HHH|dept3|  3700|\n",
      "|  9| III|dept3|  4500|\n",
      "| 10| JJJ|dept5|  3400|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from temp_table where salary >= 1500\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtP2IZlTYZz"
   },
   "source": [
    "### Leyendo la tabla HIVE como DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6Jq9GqETYZz"
   },
   "outputs": [],
   "source": [
    "# DB_NAME : Name of the the HIVE Database\n",
    "# TBL_NAME : Name of the HIVE Table\n",
    "\n",
    "\n",
    "df = spark.table(\"DB_NAME\".\"TBL_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6TI_II8TYZz"
   },
   "source": [
    "### Guardar DataFrame como tabla HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNgnXH8KTYZ0"
   },
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
    "\n",
    "## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\n",
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
    "\n",
    "# De forma predeterminada, la operación guardará el DataFrame como una tabla interna / administrada de HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "858Nw5WQTYZ0"
   },
   "source": [
    "### Guardar el DataFrame como una tabla externa HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcXaQuQhTYZ0"
   },
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYxk9Ld6TYZ0"
   },
   "source": [
    "### Crea un DataFrame a partir de un archivo CSV\n",
    "* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcCis18jTYZ0"
   },
   "outputs": [],
   "source": [
    " df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l87u7lneTYZ0"
   },
   "source": [
    "### Guardar un DataFrame como un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8fO1lAhTYZ0"
   },
   "outputs": [],
   "source": [
    "df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOCuAEobTYZ0"
   },
   "source": [
    "### Crea un DataFrame a partir de una tabla relacional\n",
    "* Podemos leer los datos de bases de datos relacionales usando una URL JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpHAlInPTYZ0"
   },
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "\n",
    "relational_df = spark.read.format('jdbc')\n",
    "                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "                        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1JH_PahTYZ1"
   },
   "source": [
    "### Guardar el DataFrame como una tabla relacional\n",
    "* Podemos guardar el DataFrame como una tabla relacional usando una URL JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dj3G9Un8TYZ1"
   },
   "outputs": [],
   "source": [
    "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
    "# TBL_NAME : Name of the relational table.\n",
    "# USER_NAME : user name to connect to DataBase.\n",
    "# PASSWORD: password to connect to DataBase.\n",
    "\n",
    "\n",
    " relational_df.write.format('jdbc')\n",
    "                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
    "                    .mode('overwrite')\n",
    "                    .save()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
